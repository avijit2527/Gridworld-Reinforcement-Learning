{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self,epsilon=0.2,alpha=0.3,gamma=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = {}\n",
    "        self.last_board = None\n",
    "        self.q_last = 0.0\n",
    "        self.state_action_last = None\n",
    "        \n",
    "        \n",
    "    def game_begin(self):\n",
    "        self.last_board = None\n",
    "        self.q_last  = 0.0\n",
    "        self.state_action_last = None\n",
    "        \n",
    "        \n",
    "    def epsilon_greedy(self, state, possible_moves):\n",
    "        self.last_board = tuple(state)\n",
    "        if(random.random() < self.epsilon):\n",
    "            move = random.choice(possible_moves)\n",
    "            self.state_action_last = (self.last_board,move)\n",
    "            self.q_last = self.getQ(self.last_board,move)\n",
    "            return move\n",
    "        else:\n",
    "            Q_list = []\n",
    "            for action in possible_moves:\n",
    "                Q_list.append(self.getQ(self.last_board,action))\n",
    "            maxQ = max(Q_list)\n",
    "            \n",
    "            if Q_list.count(maxQ) > 1:\n",
    "                best_options = [i for i in range(len(possible_moves)) if Q_list[i] == maxQ]\n",
    "                i = random.choice(best_options)\n",
    "            else:\n",
    "                i = Q_list.index(maxQ)\n",
    "            self.state_action_last = (self.last_board, possible_moves[i])\n",
    "            self.q_last = self.getQ(self.last_board, possible_moves[i])\n",
    "            return possible_moves[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def getQ(self, state, action):\n",
    "        if(self.Q.get((state,action))) is None:\n",
    "            self.Q[(state,action)] = 1.0\n",
    "        return self.Q.get((state,action))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def updateQ(self,reward, state, possible_moves):\n",
    "        q_list = []\n",
    "        for moves in possible_moves:\n",
    "            q_list.append(self.getQ(tuple(state), moves))\n",
    "        if q_list:\n",
    "            max_q_next = max(q_list)\n",
    "        else:\n",
    "            max_q_next = 0.0\n",
    "        self.Q[self.state_action_last] = self.q_last + self.alpha * ((reward + self.gamma*max_q_next) - self.q_last)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def saveQtable(self, file_name):\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(self.Q, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def loadQtable(self,file_name):\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            self.Q = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, training = False):\n",
    "        self.board = [' ']*9\n",
    "        \n",
    "        self.done = False\n",
    "        self.computer = None\n",
    "        self.training = training\n",
    "        self.player1 = None\n",
    "        self.player2 = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        if(self.training):\n",
    "            self.board = [' ']*9\n",
    "            return\n",
    "        \n",
    "    def show_board(self,board):\n",
    "        print(\"  \")\n",
    "        print(\" %c | %c | %c \"%(board[0],board[1],board[2]))\n",
    "        print(\"-%c-|-%c-|-%c-\"%('-','-','-'))\n",
    "        \n",
    "        print(\" %c | %c | %c \"%(board[3],board[4],board[5]))\n",
    "        print(\"-%c-|-%c-|-%c-\"%('-','-','-'))\n",
    "        \n",
    "        print(\" %c | %c | %c \"%(board[6],board[7],board[8]))\n",
    "        print(\"  \")\n",
    "\n",
    "        \n",
    "        \n",
    "    def evaluate(self,ch):\n",
    "        #ROW CHECKING\n",
    "        for i in range(3):\n",
    "            if(ch == self.board[i*3] == self.board[i*3 + 1] and self.board[i * 3 + 1] == self.board[i *3 + 2]):\n",
    "                return 1.0, True\n",
    "        \n",
    "        #COLUMN CHECKING\n",
    "        for i in range(3):\n",
    "            if(ch == self.board[i + 0] == self.board[i + 3] and self.board[i + 3] == self.board[i + 6]):\n",
    "                return 1.0, True\n",
    "        # DIAGONAL CHECKING\n",
    "        if (ch == self.board[0] == self.board[4] and self.board[4] == self.board[8]):\n",
    "            return 1.0, True\n",
    "\n",
    "        if (ch == self.board[2] == self.board[4] and self.board[4] == self.board[6]):\n",
    "            return 1.0, True\n",
    "        # \"if filled draw\"\n",
    "        if not any(c == ' ' for c in self.board):\n",
    "            return 0.5, True\n",
    "\n",
    "        return 0.0, False\n",
    "                \n",
    "    \n",
    "    def possible_moves(self):\n",
    "        return [moves + 1 for moves, v in enumerate(self.board) if v == ' ']\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, isX, move):\n",
    "        if(isX):\n",
    "            ch = 'X'\n",
    "        else:\n",
    "            ch = 'O'\n",
    "        if(self.board[move-1] != ' '):\n",
    "            return -5, True\n",
    "        \n",
    "        self.board[move-1] = ch\n",
    "        reward, done = self.evaluate(ch)\n",
    "        return reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def startTraining(self,player1, player2):\n",
    "        if(isinstance(player1,QLearning) and isinstance(player2, QLearning)):\n",
    "            self.training = True\n",
    "            self.player1=player1\n",
    "            self.player2=player2          \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, iterations):\n",
    "        if(self.training):\n",
    "            p1_won = 0\n",
    "            p1_draw = 0\n",
    "            p1_lose = 0\n",
    "            p1_invalid = 0\n",
    "            p2_won = 0\n",
    "            p2_draw = 0\n",
    "            p2_lose = 0\n",
    "            p2_invalid = 0\n",
    "            for i in range(iterations):\n",
    "                #print(\"Training\", i)\n",
    "                self.player1.game_begin()\n",
    "                self.player2.game_begin()\n",
    "                self.reset()\n",
    "                done = False\n",
    "                isX = random.choice([True, False])\n",
    "                while not done:\n",
    "                    if isX:\n",
    "                        move = self.player1.epsilon_greedy(self.board,self.possible_moves())\n",
    "                    else:\n",
    "                        move = self.player2.epsilon_greedy(self.board, self.possible_moves())\n",
    "                        \n",
    "                    reward, done = self.step(isX,move)\n",
    "                    \n",
    "                    if(reward == 1):\n",
    "                        if(isX):\n",
    "                            p1_won += 1\n",
    "                            self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                            self.player2.updateQ(-1 * reward, self.board, self.possible_moves())\n",
    "                        else:\n",
    "                            p2_won += 1\n",
    "                            self.player1.updateQ(-1 * reward, self.board, self.possible_moves())\n",
    "                            self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "\n",
    "                    elif (reward == 0.5):  # draw\n",
    "                        p1_draw += 1\n",
    "                        p2_draw += 1\n",
    "                        self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                        self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "\n",
    "\n",
    "                    elif (reward == -5):  # illegal move\n",
    "                        if (isX):\n",
    "                            p1_invalid += 1\n",
    "                            self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "                        else:\n",
    "                            p2_invalid += 1\n",
    "                            self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "\n",
    "                    elif (reward == 0):\n",
    "                        if (isX):  # update opposite\n",
    "                            self.player2.updateQ(reward, self.board, self.possible_moves())\n",
    "                        else:\n",
    "                            self.player1.updateQ(reward, self.board, self.possible_moves())\n",
    "\n",
    "                    isX = not isX  #\n",
    "                #self.show_board(self.board)\n",
    "\n",
    "        print(p1_won,p2_won,p1_draw,p2_draw,p1_invalid,p2_invalid)\n",
    "\n",
    "    #save Qtables\n",
    "    def saveStates(self):\n",
    "        self.player1.saveQtable(\"player1states\")\n",
    "        self.player2.saveQtable(\"player2states\")    \n",
    "        \n",
    "        \n",
    "        #save Qtables\n",
    "    def loadStates(self):\n",
    "        self.computer.loadQtable(\"player1states\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def playGame(self,computer):\n",
    "        self.computer = computer\n",
    "        self.loadStates()\n",
    "        self.computer.game_begin()\n",
    "        self.reset()\n",
    "        done = False\n",
    "        isX = random.choice([True, False])\n",
    "        while not done:\n",
    "            if isX:\n",
    "                move = self.computer.epsilon_greedy(self.board,self.possible_moves())\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                move = int(input(\"Enter Your Move: \"))\n",
    "                if move not in [1,2,3,4,5,6,7,8,9]:\n",
    "                    print(\"Invalid!\")\n",
    "                    continue\n",
    "            \n",
    "            reward, done = self.step(isX,move)\n",
    "            self.show_board(self.board)\n",
    "            if reward == -5:\n",
    "                print(\"invalid_step\")\n",
    "                done = False\n",
    "                continue\n",
    "            elif reward == 1:\n",
    "                if isX:\n",
    "                    print(\"Computer Won!\")\n",
    "                else:\n",
    "                    print(\"You Won!\")\n",
    "            elif reward == 0.5:\n",
    "                print(\"Match Drawn!\")\n",
    "            isX = not isX\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player1= QLearning() #player1 learning agent\\nplayer2 =QLearning() #player2 learning agent\\ngame.startTraining(player1,player2) #start training\\ngame.train(200000) #train for 200,000 iterations\\ngame.saveStates()  #save Qtable'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = TicTacToe(True) #game instance, True means training\n",
    "'''player1= QLearning() #player1 learning agent\n",
    "player2 =QLearning() #player2 learning agent\n",
    "game.startTraining(player1,player2) #start training\n",
    "game.train(200000) #train for 200,000 iterations\n",
    "game.saveStates()  #save Qtable'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "   |   |   \n",
      "---|---|---\n",
      " X |   |   \n",
      "---|---|---\n",
      "   |   |   \n",
      "  \n",
      "Enter Your Move: 5\n",
      "  \n",
      "   |   |   \n",
      "---|---|---\n",
      " X | O |   \n",
      "---|---|---\n",
      "   |   |   \n",
      "  \n",
      "  \n",
      " X |   |   \n",
      "---|---|---\n",
      " X | O |   \n",
      "---|---|---\n",
      "   |   |   \n",
      "  \n",
      "Enter Your Move: 7\n",
      "  \n",
      " X |   |   \n",
      "---|---|---\n",
      " X | O |   \n",
      "---|---|---\n",
      " O |   |   \n",
      "  \n",
      "  \n",
      " X | X |   \n",
      "---|---|---\n",
      " X | O |   \n",
      "---|---|---\n",
      " O |   |   \n",
      "  \n",
      "Enter Your Move: 3\n",
      "  \n",
      " X | X | O \n",
      "---|---|---\n",
      " X | O |   \n",
      "---|---|---\n",
      " O |   |   \n",
      "  \n",
      "You Won!\n"
     ]
    }
   ],
   "source": [
    "computer = QLearning(epsilon=0.2)\n",
    "game.playGame(computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference https://github.com/Rohithkvsp/Tic-Tac-Toe-Reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
