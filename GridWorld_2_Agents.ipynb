{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# @author Avijit Roy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import pprint\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_graph_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridWorld:\n",
    "    def __init__(self,epsilon=0.2,alpha=0.3,gamma=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = {}\n",
    "        self.last_grid = None\n",
    "        self.q_last = 0.0\n",
    "        self.state_action_last = None\n",
    "     \n",
    "    \n",
    "            \n",
    "    def game_begin(self):\n",
    "        self.last_grid = None\n",
    "        self.q_last  = 0.0\n",
    "        self.state_action_last = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def epsilon_greedy(self, state, possible_moves):\n",
    "        self.last_grid = state\n",
    "        if(random.random() < self.epsilon):\n",
    "            move = random.choice(possible_moves)\n",
    "            self.state_action_last = (self.last_grid,move)\n",
    "            return move\n",
    "        else:\n",
    "            Q_list = []\n",
    "            for action in possible_moves:\n",
    "                Q_list.append(self.getQ(self.last_grid,action))\n",
    "            maxQ = max(Q_list)\n",
    "            \n",
    "            if Q_list.count(maxQ) > 1:\n",
    "                best_options = [i for i in range(len(possible_moves)) if Q_list[i] == maxQ]\n",
    "                i = random.choice(best_options)\n",
    "            else:\n",
    "                i = Q_list.index(maxQ)\n",
    "            self.state_action_last = (self.last_grid, possible_moves[i])\n",
    "            self.q_last = self.getQ(self.last_grid, possible_moves[i])\n",
    "            return possible_moves[i]\n",
    "                \n",
    " \n",
    "\n",
    "        \n",
    "    def getQ(self, state, action):\n",
    "        if(self.Q.get((state,action))) is None:\n",
    "            self.Q[(state,action)] = 1.0\n",
    "        return self.Q.get((state,action))\n",
    "    \n",
    "        \n",
    "        \n",
    "    def updateQ(self, reward, state, possible_moves):\n",
    "        q_list = []\n",
    "        for moves in possible_moves:\n",
    "            q_list.append(self.getQ((state), moves))\n",
    "        if q_list:\n",
    "            max_q_next = max(q_list)\n",
    "        else:\n",
    "            max_q_next = 0.0\n",
    "        self.Q[self.state_action_last] = self.q_last + self.alpha * ((reward + self.gamma*max_q_next) - self.q_last)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def saveQtable(self, file_name):\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(self.Q, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "      \n",
    "    def loadQtable(self, file_name):\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            self.Q = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunAgents:\n",
    "    def __init__(self, training = False,beta = -5):\n",
    "        self.grid = [' '] * 81\n",
    "        \n",
    "        self.done = False\n",
    "        self.computer_1 = None\n",
    "        self.computer_2 = None\n",
    "        \n",
    "        self.training = training\n",
    "        self.agent1 = None\n",
    "        self.agent2 = None\n",
    "        self.reward_states = [30,31]\n",
    "        self.max_iter = 500\n",
    "        self.beta = beta\n",
    "        self.A_visited_states = []\n",
    "        self.B_visited_states = []\n",
    "        x = datetime.datetime.now()\n",
    "        self.time = str(x)[0:10]\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        if(self.training):\n",
    "            self.grid = [' '] * 81\n",
    "            self.grid[0] = 'A'\n",
    "            self.grid[80] = 'B'\n",
    "            self.A_visited_states = []\n",
    "            self.B_visited_states = []\n",
    "            return\n",
    "\n",
    "\n",
    "    def evaluate(self, ch):\n",
    "        \n",
    "        location = self.find_location(ch)\n",
    "        distance = self.proximity(ch)\n",
    "        proximity_reward = -math.exp(self.beta * distance)\n",
    "        i = 100\n",
    "        for x in self.reward_states:\n",
    "            i += 1\n",
    "            if location == x:\n",
    "                proximity_reward = 100 + proximity_reward   #(((np.random.randn()*i)+i) + proximity_reward )\n",
    "\n",
    "        #print(\"Reward: \",proximity_reward)    \n",
    "        return proximity_reward , False\n",
    "\n",
    "\n",
    "\n",
    "    def find_location(self,ch):\n",
    "        location = 0\n",
    "        for i,char in enumerate(self.grid):\n",
    "            if char == ch:\n",
    "                location = i\n",
    "        return location\n",
    "\n",
    "\n",
    "\n",
    "    def proximity(self,ch):\n",
    "        location_A = self.find_location('A')\n",
    "        location_B = self.find_location('B')\n",
    "        dist = abs(location_A - location_B)\n",
    "        vertical_dist = int(dist/9)\n",
    "        horizontal_dist = dist % 9\n",
    "        return  math.sqrt((vertical_dist * vertical_dist) + (horizontal_dist * horizontal_dist))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def possible_moves(self, ch):\n",
    "        location_A = self.find_location('A')\n",
    "        location_B = self.find_location('B')\n",
    "        remove = None\n",
    "        return_value = None\n",
    "        if ch == 'A':\n",
    "            location = location_A\n",
    "            if ((location_A + 1) == location_B) and (int(location_A/9) == int(location_B/9)):\n",
    "                remove = 'r'\n",
    "            elif ((location_A - 1) == location_B) and (int(location_A/9) == int(location_B/9)):\n",
    "                remove = 'l'\n",
    "            elif ((location_A + 9) == location_B):\n",
    "                remove = 'd'\n",
    "            elif ((location_A - 9) == location_B):\n",
    "                remove = 'u'\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            location = location_B\n",
    "            if ((location_B + 1) == location_A) and (int(location_A/9) == int(location_B/9)):\n",
    "                remove = 'r'\n",
    "            elif ((location_B - 1) == location_A) and (int(location_A/9) == int(location_B/9)):\n",
    "                remove = 'l'\n",
    "            elif ((location_B + 9) == location_A):\n",
    "                remove = 'd'\n",
    "            elif ((location_B - 9) == location_A):\n",
    "                remove = 'u'\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "        if location == 0:\n",
    "            return_value = ['r','d']\n",
    "        elif location == 8:\n",
    "            return_value = ['l','d']\n",
    "        elif location == 72:\n",
    "            return_value = ['u','r']\n",
    "        elif location == 80:\n",
    "            return_value = ['u','l']\n",
    "        elif location in [1,2,3,4,5,6,7]:\n",
    "            return_value = ['d','r','l']\n",
    "        elif location in [73,74,75,76,77,78,79]:\n",
    "            return_value = ['u','r','l']\n",
    "        elif location in [9,18,27,36,45,54,63]:\n",
    "            return_value = ['u','d','r']\n",
    "        elif location in [17,26,35,44,53,62,71]:\n",
    "            return_value = ['u','d','l']\n",
    "        else:\n",
    "            return_value = ['u','d','l','r']\n",
    "        if remove != None and (remove in return_value):\n",
    "            return_value.remove(remove)\n",
    "        #print(return_value)   \n",
    "        return return_value\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def is_legal(self,location, move):\n",
    "        if location == 0:\n",
    "            if move not in ['r','d']:\n",
    "                return False\n",
    "        elif location == 8:\n",
    "            if move not in ['l','d']:\n",
    "                return False\n",
    "        elif location == 72:\n",
    "            if move not in ['u','r']:\n",
    "                return False\n",
    "        elif location == 80:\n",
    "            if move not in ['u','l']:\n",
    "                return False\n",
    "        elif location in [1,2,3,4,5,6,7]:\n",
    "            if move not in ['d','r','l']:\n",
    "                return False\n",
    "        elif location in [73,74,75,76,77,78,79]:\n",
    "            if move not in ['u','r','l']:\n",
    "                return False\n",
    "        elif location in [9,18,27,36,45,54,63]:\n",
    "            if move not in ['u','d','r']:\n",
    "                return False\n",
    "        elif location in [17,26,35,44,53,62,71]:\n",
    "            if move not in ['u','d','l']:\n",
    "                return False\n",
    "        else:\n",
    "            if move not in ['u','d','l','r']:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "\n",
    "    def showQ(self,Q,iteration_number):\n",
    "        actions = ['l','r','u','d']\n",
    "        max_val_array = np.zeros((81))\n",
    "        max_act_array = np.zeros((81))\n",
    "        for x in range(81):\n",
    "            max_val = []\n",
    "            for act in actions:\n",
    "                #print(Q.get((x,act)))\n",
    "                if Q.get((x,act)):\n",
    "                    max_val.append(Q.get((x,act)))\n",
    "                else:\n",
    "                    max_val.append(0.0)\n",
    "            if len(max_val) > 0:\n",
    "                max_val_array[x] = max(max_val)\n",
    "                max_act_array[x] = np.argmax(np.array(max_val))\n",
    "\n",
    "        #print(np.reshape(max_act_array,(9,9)))\n",
    "        fig, ax = plt.subplots(figsize = (50,50))\n",
    "        plt.title(\"Q-Values \" + str(iteration_number))\n",
    "        ax.matshow(np.reshape(max_act_array,(9,9)), cmap='rainbow')\n",
    "\n",
    "        location_A = self.find_location('A')\n",
    "        location_B = self.find_location('B')\n",
    "        print(location_A)\n",
    "\n",
    "\n",
    "        for (i, j), z in np.ndenumerate(np.reshape(max_val_array,(9,9))):\n",
    "            ax.text(j, i, '%2.2f'%(z), ha='center', va='center', color = 'g',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.4'),size = 11)\n",
    "\n",
    "        #ax.text(location_A%9, int(location_A/9), 'A', ha='center', va='top', color = 'g', bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'),size = 11)\n",
    "        if not os.path.exists(\"Figure/\"+ self.time):\n",
    "            os.makedirs(\"Figure/\"+ self.time)\n",
    "        plt.savefig(\"Figure/\"+ self.time +\"/Q-Values \" + str(iteration_number))\n",
    "        #plt.draw()\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "   \n",
    "     \n",
    "    def step(self, isA, move):\n",
    "\n",
    "        if(isA):\n",
    "            ch = \"A\"\n",
    "        else:\n",
    "            ch = 'B'\n",
    "\n",
    "        location = self.find_location(ch)\n",
    "\n",
    "        if(not self.is_legal(location, move)):\n",
    "            print(\"Illegal\")\n",
    "            return -10, True\n",
    "\n",
    "        \n",
    "        self.grid[location] = ' '\n",
    "        if move == 'r' and isA:\n",
    "            self.grid[location+1] = 'A'\n",
    "        elif move == 'r':\n",
    "            self.grid[location+1] = 'B'\n",
    "        if move == 'l' and isA:\n",
    "            self.grid[location-1] = 'A'\n",
    "        elif move == 'l':\n",
    "            self.grid[location-1] = 'B'\n",
    "        if move == 'u' and isA:\n",
    "            self.grid[location-9] = 'A'\n",
    "        elif move == 'u':\n",
    "            self.grid[location-9] = 'B'\n",
    "        if move == 'd' and isA:\n",
    "            self.grid[location+9] = 'A'\n",
    "        elif move == 'd':\n",
    "            self.grid[location+9] = 'B'\n",
    "        if isA:    \n",
    "            reward, done = self.evaluate('A')\n",
    "        else:\n",
    "            reward, done = self.evaluate('B')\n",
    "            \n",
    "\n",
    "        if ch == 'A':\n",
    "            self.A_visited_states.append(self.find_location(ch))\n",
    "        else:\n",
    "            self.B_visited_states.append(self.find_location(ch))\n",
    "\n",
    "        return reward, done\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def startTraining(self, agent1, agent2):\n",
    "        if(isinstance(agent1,GridWorld) and isinstance(agent2,GridWorld)):\n",
    "            self.training = True\n",
    "            self.agent1 = agent1\n",
    "            self.agent2 = agent2\n",
    "            \n",
    "\n",
    "    \n",
    "    def train(self, iterations):\n",
    "        start_time = time.time()\n",
    "        last_time = time.time()\n",
    "        if(self.training):\n",
    "            reward_array = []\n",
    "            unique_states_visited_by_A = []\n",
    "            unique_states_visited_by_B = []\n",
    "            average_distance_per_episode_array = []    #Average distance between two agents\n",
    "            for i in range(iterations):\n",
    "                total_reward = 0\n",
    "                '''if i%20 == 0:\n",
    "                    start_time = time.time()\n",
    "                    print(\"Time Taken\")\n",
    "                    print(start_time - last_time)\n",
    "                    last_time = start_time\n",
    "                    print(i)'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                self.agent1.game_begin()\n",
    "                self.agent2.game_begin()\n",
    "                self.reset()\n",
    "                #print(self.grid)\n",
    "                done = False\n",
    "                isA = random.choice([True, False])\n",
    "                episode_length = 0\n",
    "                average_distance_per_episode = 0    #Average distance between two agents\n",
    "                while (not done) and episode_length < self.max_iter :\n",
    "                    '''if i%100 == 1:\n",
    "                        print(episode_length)\n",
    "                        self.showGrid(self.grid)\n",
    "                        time.sleep(1)'''\n",
    "                    episode_length += 1\n",
    "                    average_distance_per_episode = average_distance_per_episode + (self.proximity(\"A\")-average_distance_per_episode)/episode_length\n",
    "                    #self.showGrid(self.grid)\n",
    "                    #time.sleep(1)\n",
    "                    if isA:\n",
    "                        move = self.agent1.epsilon_greedy(self.find_location('A'), self.possible_moves('A'))\n",
    "                    else:\n",
    "                        move = self.agent2.epsilon_greedy(self.find_location('B'), self.possible_moves('B'))\n",
    "\n",
    "                    #print(self.grid)\n",
    "                    #print(move,isA) \n",
    "                    reward, done = self.step(isA,move)\n",
    "                    if(isA):\n",
    "                        total_reward += reward\n",
    "                    #print(\"Reward: \",reward)\n",
    "                    if (reward == -1):\n",
    "                        if(isA):\n",
    "                            self.agent1.updateQ(reward, self.find_location('A'), self.possible_moves('A'))\n",
    "                        else:\n",
    "                            self.agent2.updateQ(reward, self.find_location('B'), self.possible_moves('B'))\n",
    "                        \n",
    "   \n",
    "                    elif (reward == -10):\n",
    "                        if(isA):\n",
    "                            self.agent1.updateQ(reward, self.find_location('A'), self.possible_moves('A'))\n",
    "                        else:\n",
    "                            self.agent2.updateQ(reward, self.find_location('B'), self.possible_moves('B'))\n",
    "                            \n",
    "                    else:\n",
    "                        if(isA):\n",
    "                            self.agent1.updateQ(reward, self.find_location('A'), self.possible_moves('A'))\n",
    "                        else:\n",
    "                            self.agent2.updateQ(reward, self.find_location('B'), self.possible_moves('B'))\n",
    "                    \n",
    "                   \n",
    "                    isA = not isA\n",
    "\n",
    "  \n",
    "                average_distance_per_episode_array.append(average_distance_per_episode)\n",
    "                #print(\"Average distance between the agents: %f\"%(average_distance_per_episode))\n",
    "\n",
    "                unique_states_visited_by_A.append(len(set(self.A_visited_states)))\n",
    "                #print(\"Unique States Visited By A: %d\"%(len(set(self.A_visited_states))))\n",
    "\n",
    "\n",
    "                unique_states_visited_by_B.append(len(set(self.B_visited_states)))\n",
    "                #print(\"Unique States Visited By B: %d\"%(len(set(self.B_visited_states))))\n",
    "                \n",
    "\n",
    "\n",
    "                #self.showQ(self.agent1.Q,i)\n",
    "                reward_array.append(total_reward)                      \n",
    "                #print(\"End of Epoch\")\n",
    "\n",
    "\n",
    "            print(distance_graph_array)\n",
    "            distance_graph_array.append([np.mean(average_distance_per_episode_array),self.beta])\n",
    "            print(\"Average distance per run: %f \"%(np.mean(average_distance_per_episode_array)))\n",
    "            self.plot_average_distance(average_distance_per_episode_array,np.mean(average_distance_per_episode_array),i+1)\n",
    "            #plt.plot(average_distance_per_episode_array)\n",
    "            #plt.axhline(y=np.mean(average_distance_per_episode_array),c=\"red\")\n",
    "            #plt.show()\n",
    "            mean_A = np.mean(unique_states_visited_by_A)\n",
    "            mean_B = np.mean(unique_states_visited_by_B)\n",
    "            #self.plot_unique_states_visited(unique_states_visited_by_A,mean_A ,'A')\n",
    "            #self.plot_unique_states_visited(unique_states_visited_by_B,mean_B ,'B')\n",
    "            #plt.scatter(range(len(reward_array)),reward_array)\n",
    "            #plt.show()\n",
    "               \n",
    " \n",
    "\n",
    "    def plot_unique_states_visited(self,array,mean,ch):\n",
    "        plt.clf()\n",
    "        plt.plot(array)\n",
    "        plt.axhline(y=mean,c=\"red\")\n",
    "        plt.title(\"Number of Unique States Visited by Agent \" + ch +\" and Beta:\" + str(self.beta)[0:7])\n",
    "        plt.ylabel(\"Number of States\")\n",
    "        plt.xlabel(\"Episode Number\")\n",
    "        if not os.path.exists(\"Figure/\"+ self.time):\n",
    "            os.makedirs(\"Figure/\"+ self.time)\n",
    "        plt.savefig(\"./Figure/\" + self.time + \"/Agent_\"+ ch +\"_BetaValue_\" + str(self.beta)[0:7] + \".png\")\n",
    "        \n",
    "\n",
    "\n",
    "    def plot_average_distance(self,array,mean,iteration):\n",
    "        plt.clf()\n",
    "        plt.plot(array)\n",
    "        plt.axhline(y=mean,c=\"red\")\n",
    "        plt.title(\"Distance between two Agents for iter \" + str(iteration) +\" and Beta:\" + str(self.beta)[0:7])\n",
    "        plt.ylabel(\"Distance\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        if not os.path.exists(\"Figure/\"+ self.time):\n",
    "            os.makedirs(\"Figure/\"+ self.time)\n",
    "        plt.savefig(\"./Figure/\" + self.time + \"/Iter_\"+ str(iteration) +\"_BetaValue_\" + str(self.beta)[0:7] + \".png\")\n",
    "\n",
    "\n",
    "    \n",
    "    def showGrid(self, grid):\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"------------------------------------\")\n",
    "        pprint.pprint(grid[0:9])\n",
    "        pprint.pprint(grid[9:18])\n",
    "        pprint.pprint(grid[18:27])\n",
    "        pprint.pprint(grid[27:36])\n",
    "        pprint.pprint(grid[36:45])\n",
    "        pprint.pprint(grid[45:54])\n",
    "        pprint.pprint(grid[54:63])\n",
    "        pprint.pprint(grid[63:72])\n",
    "        pprint.pprint(grid[72:81])\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def playComputer(self, agent1, agent2):\n",
    "        self.computer_1 = agent1\n",
    "        self.computer_2 = agent2\n",
    "        self.loadStates()\n",
    "        self.computer_1.game_begin()\n",
    "        self.computer_2.game_begin()\n",
    "        self.reset()\n",
    "        done = False\n",
    "        isA = random.choice([True, False])\n",
    "        episode_length = 0\n",
    "        while (not done) and episode_length < self.max_iter :\n",
    "            print(episode_length)\n",
    "            self.showGrid(self.grid)\n",
    "            time.sleep(1)\n",
    "            episode_length += 1   \n",
    "            if isA:\n",
    "                move = self.agent1.epsilon_greedy(self.find_location('A'), self.possible_moves('A'))\n",
    "            else:\n",
    "                move = self.agent2.epsilon_greedy(self.find_location('B'), self.possible_moves('B'))\n",
    "            reward, done = self.step(isA,move)\n",
    "            isA = not isA\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    def saveStates(self):\n",
    "        self.agent1.saveQtable(\"agent1states\")\n",
    "        self.agent2.saveQtable(\"agent2states\")    \n",
    "        \n",
    "        \n",
    "        #save Qtables\n",
    "    def loadStates(self):\n",
    "        self.computer_1.loadQtable(\"agent1states\")\n",
    "        self.computer_2.loadQtable(\"agent2states\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta_array = np.linspace(-20,20,num=50)\n",
    "for beta in beta_array:\n",
    "    print(beta)\n",
    "    game = RunAgents(True,beta)\n",
    "    agent1 = GridWorld(epsilon = 0.2)\n",
    "    agent2 = GridWorld(epsilon = 0.2)\n",
    "    game.startTraining(agent1,agent2)\n",
    "    game.train(100) #train for 200,000 iterations\n",
    "    game.saveStates() \n",
    "\n",
    "\n",
    "distance_graph_array = np.array(distance_graph_array)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(distance_graph_array.T[1],distance_graph_array.T[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0,105)\n",
    "ax.set_ylim(0,12)\n",
    "line, = ax.plot(0,0)\n",
    "\n",
    "def animation_frame(i):\n",
    "    x_data.append(i*10)\n",
    "    y_data.append(i)\n",
    "    \n",
    "    line.set_xdata(x_data)\n",
    "    line.set_ydata(y_data)\n",
    "    return line\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig,func = animation_frame, frames = np.arange(0,10,0.01), interval = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
